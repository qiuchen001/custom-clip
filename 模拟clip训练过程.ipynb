{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. 图像和文本特征提取\n",
    "使用图像编码器（如 ResNet 或视觉 Transformer）对图像进行编码，得到图像特征向量I_f\n",
    "使用文本编码器（如 CBOW 或文本 Transformer）对文本进行编码，得到文本特征向量T_f"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c281e274efebc2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\soft\\New\\miniconda\\envs\\torch-play\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\soft\\New\\miniconda\\envs\\torch-play\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 图像编码器\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # 去掉全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# 文本编码器\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state[:, 0, :]  # 取 [CLS] 标记的嵌入\n",
    "\n",
    "# 示例数据\n",
    "image_encoder = ImageEncoder()\n",
    "text_encoder = TextEncoder()\n",
    "\n",
    "# 图像数据\n",
    "image_data = torch.randn(16, 3, 224, 224)  # 16 张 224x224 的图像\n",
    "image_features = image_encoder(image_data)\n",
    "\n",
    "# 文本数据\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "texts = [\"This is a sample text.\"] * 16\n",
    "input_ids = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')['input_ids']\n",
    "attention_mask = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')['attention_mask']\n",
    "text_features = text_encoder(input_ids, attention_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T03:09:33.983772400Z",
     "start_time": "2024-09-29T03:09:26.229359200Z"
    }
   },
   "id": "b406d21f00dcffcc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 投影到共同嵌入空间\n",
    "将图像特征I_f和文本特征T_f投影到嵌入空间，并进行 L2 归一化。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5dc57f9f8a895e8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 投影头\n",
    "image_projection_head = ProjectionHead(image_features.shape[1], 512) # 投影头是一个全连接层（线性层），它将输入特征的维度映射到指定的输出维度。投影头将输入特征的维度 input_dim 映射到 512 维的嵌入空间。投影头的作用是将高维的图像特征和文本特征映射到一个低维的嵌入空间，以便进行相似度计算。通过投影头，可以将不同模态（图像和文本）的特征映射到同一个嵌入空间，从而方便计算它们之间的相似度。\n",
    "text_projection_head = ProjectionHead(text_features.shape[1], 512)\n",
    "\n",
    "# 投影并归一化\n",
    "image_embeddings = nn.functional.normalize(image_projection_head(image_features), dim=1) # 使用投影头将图像特征和文本特征投影到嵌入空间，并进行 L2 归一化。使用 nn.functional.normalize 函数对投影后的特征进行 L2 归一化，使得每个特征向量的 L2 范数为 1。L2 归一化可以统一特征向量的尺度，提高相似度计算的稳定性和模型的泛化能力\n",
    "text_embeddings = nn.functional.normalize(text_projection_head(text_features), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T03:11:49.754342300Z",
     "start_time": "2024-09-29T03:11:49.735344200Z"
    }
   },
   "id": "87ecc74f200d7bd0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 计算相似性分数\n",
    "通过计算图像和文本嵌入的余弦相似度，得到相似性矩阵。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8420a5207454e496"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 计算相似性矩阵\n",
    "logits = torch.matmul(image_embeddings, text_embeddings.T)\n",
    "logits = logits * torch.exp(torch.tensor(1.0))  # 温度参数 t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T03:12:11.128017100Z",
     "start_time": "2024-09-29T03:12:11.110170400Z"
    }
   },
   "id": "1c84627a4ccdb50f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. 构建损失函数\n",
    "使用交叉熵损失分别对图像和文本进行监督。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b32ec9dc4190f0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 标签\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39marange(logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# 对图像的损失\u001B[39;00m\n\u001B[0;32m      5\u001B[0m loss_i \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, labels) \u001B[38;5;66;03m# 这里的损失不仅包括正确匹配的图像和文本对的损失，还包括不匹配的图像和文本对的损失。nn.CrossEntropyLoss 的输入是未归一化的 logits（即未经过 softmax 的输出），而不是经过 softmax 的概率分布。nn.CrossEntropyLoss 内部会自动对 logits 进行 softmax 操作，然后计算交叉熵损失。\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# 标签\n",
    "labels = torch.arange(logits.shape[0])\n",
    "\n",
    "# 对图像的损失\n",
    "loss_i = F.cross_entropy(logits, labels) # 这里的损失不仅包括正确匹配的图像和文本对的损失，还包括不匹配的图像和文本对的损失。nn.CrossEntropyLoss 的输入是未归一化的 logits（即未经过 softmax 的输出），而不是经过 softmax 的概率分布。nn.CrossEntropyLoss 内部会自动对 logits 进行 softmax 操作，然后计算交叉熵损失。\n",
    "\n",
    "# 对文本的损失\n",
    "loss_t = F.cross_entropy(logits.T, labels)\n",
    "\n",
    "# 最终损失\n",
    "loss = (loss_i + loss_t) / 2 # 为什么求这两部分的平均值：通过计算两部分损失的平均值，可以平衡图像和文本的匹配情况，使得模型在图像和文本的匹配任务上都能得到良好的表现"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T02:25:31.692988100Z",
     "start_time": "2024-10-11T02:25:31.053481100Z"
    }
   },
   "id": "9ac90272471c3590"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 在深度学习中，logits 和 softmax 是两个密切相关的概念，尤其是在分类任务中。\n",
    "1. Logits\n",
    "定义: logits 是指神经网络最后一层（通常是全连接层）输出的原始分数（未经过任何激活函数处理的值）。\n",
    "形状: 对于一个分类问题，假设有 C 个类别，logits 的形状通常是 (batch_size, C)，其中 batch_size 是输入样本的数量，C 是类别的数量。\n",
    "特点: logits 是未归一化的值，表示模型对每个类别的“信心”或“分数”。这些分数可以是任意实数，不限于 [0, 1] 区间。\n",
    "2. Softmax\n",
    "定义: softmax 是一种激活函数，用于将 logits 转换为概率分布。softmax 函数的输出是一个概率分布，表示模型对每个类别的预测概率。\n",
    "公式: 对于一个样本的 logits 向量 z = [z1, z2, ..., zC]，softmax 的输出 p = [p1, p2, ..., pC] 计算如下：\n",
    "其中 e 是自然对数的底数。\n",
    "特点: softmax 的输出是一个概率分布，即所有 pi 的和为 1，并且每个 pi 都在 [0, 1] 区间内。\n",
    "3. 关系\n",
    "顺序: logits 是 softmax 的输入，softmax 是 logits 的输出。\n",
    "用途: 在分类任务中，logits 通常作为 softmax 的输入，softmax 的输出用于计算交叉熵损失（如 nn.CrossEntropyLoss）。\n",
    "计算: nn.CrossEntropyLoss 内部会自动对 logits 进行 softmax 操作，然后计算交叉熵损失。因此，在使用 nn.CrossEntropyLoss 时，直接传入 logits 即可，不需要手动进行 softmax 操作。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce4871587e8d36a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. 优化目标\n",
    "最大化对角线元素的相似性（匹配的图像-文本对），最小化非对角线元素的相似性（不匹配的图像-文本对）。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "454daafadac1d53d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': image_encoder.parameters()},\n",
    "    {'params': text_encoder.parameters()},\n",
    "    {'params': image_projection_head.parameters()},\n",
    "    {'params': text_projection_head.parameters()}\n",
    "], lr=1e-4)\n",
    "\n",
    "# 反向传播和优化\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T03:12:57.962120100Z",
     "start_time": "2024-09-29T03:12:55.956575200Z"
    }
   },
   "id": "7d6c374bfb10a613"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss: 2.773303985595703\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T03:15:16.704096900Z",
     "start_time": "2024-09-29T03:15:16.693098100Z"
    }
   },
   "id": "ad05af847ff5f409"
  },
  {
   "cell_type": "markdown",
   "source": [
    "总结\n",
    "CLIP 模型的训练过程可以总结为以下几个步骤：\n",
    "1. 图像和文本特征提取：\n",
    "- 使用图像编码器对图像进行编码，得到图像特征向量 \n",
    "- 使用文本编码器对文本进行编码，得到文本特征向量 \n",
    "2. 投影到共同嵌入空间：\n",
    "- 将图像特征和文本特征投影到嵌入空间，并进行 L2 归一化。\n",
    "3. 计算相似性分数：\n",
    "- 通过计算图像和文本嵌入的余弦相似度，得到相似性矩阵。\n",
    "4. 构建损失函数：\n",
    "- 使用交叉熵损失分别对图像和文本进行监督。\n",
    "- 最终损失为两者的平均。\n",
    "5. 优化目标：\n",
    "- 最大化对角线元素的相似性（匹配的图像-文本对）。\n",
    "- 最小化非对角线元素的相似性（不匹配的图像-文本对）。\n",
    "\n",
    "通过这些步骤，可以实现 CLIP 模型的训练过程，使得在嵌入空间中，正确匹配的图像和文本对的相似度更高。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "628abd4db40a77f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
